# 4장. 데이터 수집: 데이터 추출

생성일: 2022년 3월 11일 오후 10:09

3장 에서 논의한 바와 같이 ELT 패턴은 데이터 분석, 데이터 과학 및 데이터 제품을 위해 구축된 데이터 파이프라인에 이상적인 설계이다. ELT 패턴의 처음 두 단계인 **추출 및 로드를 집합적으로 *데이터* 수집**이라고 한다. 이 장에서는 개발 환경과 인프라를 둘 다 설정하는 방법에 대해 설명하고 다양한 소스 시스템에서 데이터를 추출하는 방법에 대해 설명한다. 5장 에서는 결과 데이터 세트를 데이터 웨어하우스에 로드하는 방법에 대해 설명한다.

<aside>
💡 **메모**
이 장의 추출 및 로드 코드 샘플은 서로 완전히 분리되어 있다. 데이터 수집을 완료하기 위해 두 단계를 조정하는 것은 7장 에서 논의되는 주제이다.

</aside>

2장 에서 논의한 것처럼 추출할 소스 시스템의 유형과 로드할 대상이 많이 있다. 또한 데이터는 다양한 형태로 제공되며 모든 데이터를 수집하는 데 서로 다른 여러 문제도 있다.

이 장과 다음 장에는 공통 시스템에서 데이터를 내보내고 수집하기 위한 코드 샘플이 포함되어 있다. 코드는 매우 단순하며 최소한의 오류 처리만 포함한다. 각 예는 데이터 수집을 위한 이해하기 쉬운 시작점으로 의도되었지만 완전히 작동하고 더 확장 가능한 솔루션으로 확장할 수 있다. 

5장 에서는 구축할 수 있는 몇 가지 오픈 소스 프레임워크와 데이터 엔지니어와 분석가에게 데이터 수집을 위한 "로우 코드" 옵션을 제공하는 상업적 대안에 대해서도 설명한다.

<aside>
💡 **메모**
이 장의 코드 샘플은 추출된 데이터를 대상 데이터 웨어하우스에 로드할 CSV 파일에 쓴다. 로드하기 전에 추출된 데이터를 JSON과 같은 다른 형식으로 저장하는 것이 더 합리적인 경우도 있다. 해당되는 경우 그러한 조정을 고려할 수 있는 위치를 알려준다.

</aside>

---

### Python 환경 설정

이어지는 모든 코드 샘플은 Python 및 SQL로 작성되었으며 오늘날 데이터 엔지니어링 분야에서 일반적으로 사용되는 오픈 소스 프레임워크를 사용한다. 간단하게 하기 위해 소스와 대상의 수는 제한되어 있다. 그러나 해당되는 경우 유사한 시스템을 수정하는 방법에 대한 메모를 제공한다.

샘플 코드를 실행하려면 Python 3.x를 실행하는 물리적 또는 가상 머신이 필요하다. 또한 몇 가지 라이브러리를 설치하고 가져와야 한다.

<aside>
💡 **메모**
다음 명령은 Linux 또는 Macintosh 명령줄에 대해 작성되었다. Windows에서는 Python 3 실행 파일을 PATH에 추가해야 할 수 있다.

</aside>

*이 장에서 사용하는 라이브러리를 설치하기 전에 라이브러리를 설치할 가상 환경* 을 만드는 것이 가장 좋다. 이렇게 하려면 `virtualenv`이라는 도구를 사용할 수 있다. `virtualenv` 은 다양한 프로젝트 및 응용 프로그램에 대한 Python 라이브러리를 관리하는 데 유용하다. 이를 통해 전역이 아닌 프로젝트에 특정한 범위 내에서 Python 라이브러리를 설치할 수 있다.

1. *먼저 env* 라는 가상 환경을 만든다 .

```
$ python -m venv env
```

1. 이제 가상 환경이 생성되었으므로 다음 명령을 사용하여 활성화한다.

```
$ source env/bin/activate
```

1. 두 가지 방법으로 가상 환경이 활성화되었는지 확인할 수 있다. 
먼저 명령 프롬프트에 환경 이름이 접두사로 붙는 것을 알 수 있다.

```
(env) $
```

1. `which python`명령을 사용하여 Python이 라이브러리를 찾는 위치를 확인할 수도 있다 . 
다음과 같이 가상 환경 디렉토리의 경로를 보여주는 것을 볼 수 있다 .

```
(env) $ which python
env/bin/python
```

이제 다음 코드 샘플에 필요한 라이브러리를 안전하게 설치할 수 있다.

<aside>
💡 **메모**
일부 운영 체제(OS)에서는 Python 3.x 실행 파일을 실행하는 `python3` 을 사용해야 한다. 
이전 OS 버전은 기본적으로 Python 2.x일 수 있다. 
`python --version` 을 입력하여 OS에서 사용하는 Python 버전을 확인할 수 있다.

</aside>

이 장 전체 `pip`에서 코드 샘플에 사용된 라이브러리를 설치하는 데 사용할 것이다. 

- 설치할 첫 번째 라이브러리 `**configparser` 은 나중에 파일에 추가할 구성 정보를 읽는 데 사용**된다.

```
(env) $ pip install configparser
```

- 다음으로, 생성할 Python 스크립트와 동일한 디렉터리에 *pipeline.conf* 라는 파일을 생성 한다. 지금은 파일을 비워 둔다.  Linux 및 Mac 운영 체제에서는 다음 명령을 사용하여 명령줄에 빈 파일을 만들 수 있다.

```
(env) $ touch pipeline.conf
```

<aside>
💡 ****GIT 저장소에 구성 파일을 추가하지 마시오!****
구성 파일에 자격 증명과 연결 정보를 저장하게 되므로 Git 리포지토리에 추가하지 마시오. 이 정보는 S3 버킷, 소스 시스템 및 데이터 웨어하우스에 액세스할 수 있는 안전하고 승인된 시스템에만 로컬로 저장해야 한다. **리포지토리에서 제외를 보장하는 가장 안전한 방법은 구성 파일에 *.conf 와 같은 확장자를 지정하고 *.conf 를 사용하여 .gitignore* 파일 에 줄을 추가하는 것 이다.**

</aside>

---

### 클라우드 파일 스토리지 설정

이 장의 각 예에서는 Amazon Simple Storage Service(Amazon S3 또는 간단히 S3) 버킷을 사용한다. 파일 저장을 위해. S3는 AWS에서 호스팅되며, 이름에서 알 수 있듯이 S3는 파일을 저장하고 액세스하는 간단한 방법이다. 또한 매우 비용 효율적이다. 이 글을 쓰는 시점에서 AWS는 새 AWS 계정으로 12개월 동안 5GB의 무료 S3 스토리지를 제공하고 그 이후에는 표준 S3 스토리지 클래스에 대해 기가바이트당 월 3센트 미만의 요금을 부과한다. 이 장의 샘플이 간단하다는 점을 감안할 때 AWS 계정을 생성한 첫 12개월 동안 또는 그 후 한 달에 1달러 미만인 경우 필요한 데이터를 S3에 무료로 저장할 수 있다.

**이 장의 샘플을 실행하려면 S3 버킷이 필요하다.** 고맙게도 S3 버킷 생성은 간단하며 최신 지침은 [AWS 설명서](https://oreil.ly/7W9ZD) 에서 찾을 수 있다. S3 버킷에 대한 적절한 액세스 제어를 설정하는 것은 사용 중인 데이터 웨어하우스에 따라 다르다. 일반적으로 액세스 관리 정책에는 AWS Identity and Access Management(IAM) 역할을 사용하는 것이 가장 좋다. Amazon Redshift 및 Snowflake 데이터 웨어하우스 모두에 대해 이러한 액세스를 설정하는 자세한 지침은 다음 섹션에 있다. 지금은 지침에 따라 새 버킷을 생성하시오. 원하는 대로 이름을 지정하십시오. **버킷을 비공개로 유지하는 것을 포함하여 기본 설정을 사용하는 것이 좋다.**

각 추출 예제는 주어진 소스 시스템에서 데이터를 추출하고 출력을 S3 버킷에 저장한다. 5장의 각 로드 예제에서는 해당 데이터를 S3 버킷에서 대상으로 로드한다. 이것은 데이터 파이프라인의 일반적인 패턴이다. 모든 주요 공용 클라우드 제공업체에는 S3와 유사한 서비스가 있는데, 다른 공용 클라우드에 해당하는 것은 Microsoft Azure의 Azure Storage 및 GCP의 Google Cloud Storage(GCS)이다.

각 예제로 이동하기 전에 추출 및 로드를 위한 스크립트가 S3 버킷과 상호 작용할 수 있도록 **Boto3를 설치 해야한다. 이는 Python용 AWS SDK**이다. 이전 섹션에서 설정한 가상 환경이 활성 상태인지 확인하고 `pip`를 설치하는 데 사용한다.

- `boto3` 설치

```
(env) $ pip install boto3
```

- 설치한 `boto3` 을 Python 스크립트로 가져오도록 요청하는 코드

```
import boto3
```

 

`boto3` Python 라이브러리를 사용하여 S3 버킷과 상호 작용할 것이므로 IAM 사용자를 생성하고 해당 사용자에 대한 액세스 키를 생성하고 Python 스크립트가 읽을 수 있는 구성 파일에 키를 저장해야 한다. 이는 스크립트가 S3 버킷의 파일을 읽고 쓸 수 있는 권한을 갖도록 하기 위해 모두 필요하다.

1. 먼저 IAM 사용자를 생성.
2. AWS 콘솔(또는 상단 탐색 모음)의 서비스 메뉴에서 IAM으로 이동한다.
3. 탐색 창에서 사용자를 클릭한 다음 "사용자 추가"를 클릭 한다. 새 사용자의 사용자 이름을 입력 한다. 이 예에서 사용자 이름을 *data_pipeline_readwrite* 로 지정 한다.
4. 이 IAM 사용자의 액세스 유형을 클릭 한다. 이 사용자는 AWS 콘솔에 로그인할 필요가 없고 Python 스크립트를 통해 프로그래밍 방식으로 AWS 리소스에 액세스할 수 있으므로 "프로그래밍 방식 액세스"를 클릭.
5. 다음: 권한을 클릭.
6. "권한 설정" 페이지에서 "기존 정책을 사용자에게 직접 연결" 옵션을 클릭. AmazonS3FullAccess 정책을 추가한다.
7. 다음: 태그를 클릭. AWS에서는 나중에 찾을 수 있도록 다양한 객체 및 서비스에 태그를 추가하는 것이 모범 사례이나 이것은 선택 사항이다.
8. 다음: 검토를 클릭하여 설정을 확인. 모든 것이 좋아 보이면 "사용자 만들기"를 클릭.
9. 새 IAM 사용자의 액세스 키 ID와 보안 액세스 키를 저장하려고 한다. 이렇게 하려면 Download.csv를 클릭한 다음 파일을 안전한 위치에 저장하여 잠시 후에 사용할 수 있다.

마지막으로 IAM 사용자의 자격 증명과 S3 버킷 정보를 저장하기 위해 호출 되는 **`[aws_boto_credentials]` *섹션을* *pipeline.conf 파일에 추가한다.* AWS 사이트에 로그인할 때 페이지 오른쪽 상단에 있는 계정 이름을 클릭하면 AWS 계정 ID를 찾을 수 있다. `bucket_name`값 에 대해 이전에 생성한 S3 버킷의 이름을 사용한다.

- *pipline.conf* 의 새 섹션 은 다음과 같다.

```
[aws_boto_credentials]
access_key = ijfiojr54rg8er8erg8erg8
secret_key = 5r4f84er4ghrg484eg84re84ger84
bucket_name = pipeline-bucket
account_id = 4515465518
```

---

### MySQL 데이터베이스에서 데이터 추출

데이터 추출MySQL 데이터베이스에서 두 가지 방법으로 수행할 수 있다.

- **SQL을 사용한 전체 또는 증분 추출**
: 구현하기가 훨씬 간단하지만 자주 변경되는 대규모 데이터 세트의 경우 확장성이 떨어진다.
- **바이너리 로그(binlog) 복제**
: 구현하기가 더 복잡하지만 소스 테이블의 변경 데이터 볼륨이 높거나 MySQL 소스에서 더 자주 데이터를 수집해야 하는 경우에 더 적합하다.

이 섹션은 데이터를 추출해야 하는 MySQL 데이터 소스가 있는 독자와 관련이 있으나 코드 샘플을 시도할 수 있도록 간단한 데이터베이스를 설정하려는 경우 두 가지 옵션이 있다. 먼저 로컬 머신이나 가상 머신에 MySQL을 무료로 설치할 수 있다. [MySQL 다운로드 페이지](https://oreil.ly/p2-L1) 에서 OS용 설치 프로그램을 찾을 수 있다. [또는 AWS](https://oreil.ly/XahtN) 에서 MySQL용 완전 관리형 Amazon RDS 인스턴스를 생성할 수 있다 . 이 방법이 더 간단하고 로컬 컴퓨터에 불필요한 어수선함을 만들지 않는 것이 좋다!

<aside>
💡 **경고**
연결된 지침 에 따라 MySQL RDS 데이터베이스 인스턴스를 설정하면 데이터베이스를 공개적으로 액세스할 수 있도록 설정하라는 메시지가 표시된다. 샘플 데이터를 학습하고 작업하는 데 적합하다. 사실, 이 섹션에서 샘플을 실행하는 모든 컴퓨터에서 연결하는 것이 훨씬 더 쉽다. 그러나 프로덕션 환경에서 보다 강력한 보안을 위해 [Amazon RDS 보안 모범 사례](https://oreil.ly/8DYsz) 를 따르는 것이 좋다 .

</aside>

이 섹션의 코드 샘플은 매우 간단하며 MySQL 데이터베이스에 `Orders` 로 명명된 테이블을 참조한다. 작업할 MySQL 인스턴스가 있으면 다음 SQL 명령 을 실행하여 테이블을 만들고 일부 샘플 행을 삽입할 수 있다 .

```sql
CREATE TABLE Orders (
  OrderId int,
  OrderStatus varchar(30),
  LastUpdated timestamp
);

INSERT INTO Orders
  VALUES(1,'Backordered', '2020-06-01 12:00:00');
INSERT INTO Orders
  VALUES(1,'Shipped', '2020-06-09 12:00:25');
INSERT INTO Orders
  VALUES(2,'Shipped', '2020-07-11 3:05:00');
INSERT INTO Orders
  VALUES(1,'Shipped', '2020-06-09 11:50:00');
INSERT INTO Orders
  VALUES(3,'Shipped', '2020-07-12 12:00:00');
```

### **전체 또는 증분 MySQL 테이블 추출**

MySQL 테이블의 열 전체 또는 일부를 데이터 웨어하우스 또는 데이터 레이크로 수집해야 하는 경우 전체 추출 또는 증분 추출을 사용하여 이를 수행할 수 있다.

1. ***전체 추출*** 
    - 추출 작업을 실행할 때마다 테이블의 모든 레코드가 추출된다.
    - 이것은 가장 덜 복잡한 접근 방식이지만 대용량 테이블의 경우 실행하는 데 오랜 시간이 걸릴 수 있다.
    - 예를 들어 `Orders`라는 테이블에서 전체 추출을 실행하려는 경우 원본 MySQL 데이터베이스에서 실행되는 SQL은 다음과 같다.

```sql
SELECT *
FROM Orders;
```

1. ***증분 추출*** 
    - 작업의 마지막 실행 이후에 변경되거나 추가된 원본 테이블의 레코드만 추출된다.
    - 마지막 추출의 타임스탬프는 데이터 웨어하우스의 추출 작업 로그 테이블에 저장하거나,
    웨어하우스의 대상 테이블에 있는  `LastUpdated`열의 최대 타임스탬프를 쿼리하여 검색할 수 있다.
    - 가상의 `Orders`테이블을 예로 사용하면 원본 MySQL 데이터베이스에서 실행되는 SQL 쿼리는 다음과 같다.
    - `{{ last_extraction_run }}`변수는 추출 작업의 가장 최근 실행을 나타내는 타임스탬프이다.
    이 경우 다음 SQL이 데이터 웨어하우스에서 실행되고 결과 값이 `{{ last_extraction_run }}`에 사용된다.

```sql
SELECT *
FROM Orders
WHERE LastUpdated > {{last_extraction_run}};
```

<aside>
💡 **메모**
*변경할 수 없는 데이터* 가 포함된 테이블 의 경우 (즉, 레코드를 삽입할 수 있지만 업데이트할 수 없음) `LastUpdated`열 대신 레코드가 생성된 시간에 대한 타임스탬프를 사용할 수 있다.

</aside>

****마지막 업데이트 날짜 캐싱****

`Orders` 테이블이 상당히 크면 추출 작업의 다음 실행에서 빠르게 쿼리할 수 있는 로그 테이블에 마지막 업데이트된 레코드의 값을 저장할 수 있다. 추출 작업이 시작되거나 완료된 시간이 아니라 대상 테이블의 값`MAX(LastUpdated)`을 데이터 웨어하우스에 저장해야 한다.

```sql
SELECT MAX(LastUpdated)
FROM warehouse.Orders;
```

**증분 추출이 최적의 성능에 이상적이지만 주어진 테이블에 대해 불가능할 수 있는** **몇 가지 단점과 이유가 있다.** 

1. 이 method를 삭제하면 행이 캡처되지 않는다. 원본 MySQL 테이블에서 행이 삭제되면 알 수 없으며 아무 것도 변경되지 않은 것처럼 대상 테이블에 남아 있다.
2.  원본 테이블에는 마지막으로 업데이트된 시간에 대한 신뢰할 수 있는 타임스탬프(`LastUpdated` 와 같은 이전 예에 나왔던 열) 가 있어야 한다. 소스 시스템 테이블에 이러한 열이 없거나 안정적으로 업데이트되지 않는 열이 있는 경우가 많다. 

그러나 증분 추출을 사용하면 업데이트된 행을 더 쉽게 캡처할 수 있다. 향후 코드 샘플에서  `Orders`테이블의 특정 행이 업데이트 되면 전체 및 증분 추출이 모두 최신 버전의 행을 가져온다. 전체 추출에서는 추출이 테이블의 전체 복사본을 검색하므로 테이블의 모든 행에 대해 마찬가지이다. 증분 추출에서는 변경된 행만 검색된다.

로드 단계에서 일반적으로 대상 테이블을 먼저 자르고 새로 추출된 데이터를 로드하여 전체 추출을 로드한다. 
이 경우 데이터 웨어하우스에 최신 버전의 행만 남게 된다. 증분 추출에서 데이터를 로드할 때 결과 데이터가 대상 테이블의 데이터에 추가된다. 이 경우 원본 레코드와 업데이트된 버전이 모두 있게 된다. 6장 에서 논의한 것처럼 데이터를 변환하고 분석할 때 두 가지 모두를 갖는 것이 유용하다.

예를 들어, *표 4-1*은 MySQL 데이터베이스의 `OrderId` 1에 대한 원래 레코드를 보여준다. 고객이 주문했을 때 이월 주문 상태다. *표 4-2* 는 MySQL 데이터베이스에서 업데이트된 레코드를 보여준다. 보시다시피 2020-06-09에 배송되었기 때문에 주문이 업데이트 되었다.

[표 4-1. OrderId 1의 원래 상태](https://www.notion.so/e7911fb086d34588b73f156c79ea7f7f)

[표 4-2. OrderId 1의 업데이트된 상태](https://www.notion.so/2f587b4951fa4fe9b36dbd3c962c1f3f)

전체 추출이 실행되면 데이터 웨어하우스의 대상 테이블이 먼저 잘린 다음 추출 출력과 함께 로드된다. 
`OrderId`1에 대한 결과 는 *표 4-2* 에 표시된 단일 레코드다. 

그러나 증분 추출에서 추출의 출력은 단순히 데이터 웨어하우스의 대상 테이블에 추가된다. 결과는 표 4-3 에 나와 있는 것처럼 데이터 웨어하우스에 있는 `OrderId`1에 대한 원본 및 업데이트된 레코드다.

[표 4-3. 데이터 웨어하우스에 있는 모든 버전의 OrderId 1](https://www.notion.so/740ced79435d42d88b2f63dd771bd08b)

MySQL 데이터베이스에서 전체 및 증분 추출은 모두 데이터베이스에서 실행되지만 Python 스크립트에 의해 트리거되는 SQL 쿼리를 사용하여 구현할 수 있다. 따라서 `PyMySQL`라이브러리 를 설치해야 한다.

```
(env) $ pip install pymysql
```

또한 MySQL 데이터베이스에 대한 연결 정보를 저장하기 위해 *pipeline.conf* 파일 에 새 섹션을 추가해야 한다.

```
[mysql_config]
hostname = my_host.com
port = 3306
username = my_user_name
password = my_password
database = db_name
```

*이제 extract_mysql_full.py* 라는 새 Python 스크립트를 만든다. 로드 단계에서 데이터 웨어하우스로 가져오기 쉬운 플랫 파일에 추출된 데이터를 구조화하고 쓸 수 있도록 MySQL 데이터베이스에 연결하는 ,`csv` `pymysql` 라이브러리와 같은 여러 라이브러리를 가져와야 한다. 또한 나중에 데이터 웨어하우스에 로드하기 위해 결과 CSV 파일을 S3 버킷에 업로드할 수 있도록 `boto3` 도 가져온다.

```python
import pymysql
import csv
import boto3
import configparser
```

이제 MySQL 데이터베이스에 대한 연결을 초기화할 수 있다.

```python
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysql_config", "hostname")
port = parser.get("mysql_config", "port")
username = parser.get("mysql_config", "username")
dbname = parser.get("mysql_config", "database")
password = parser.get("mysql_config", "password")

conn = pymysql.connect(host=hostname,
        user=username,
        password=password,
        db=dbname,
        port=int(port))

if conn is None:
  print("Error connecting to the MySQL database")
else:
  print("MySQL connection established!")
```

이전 예에서 `Orders`테이블 의 전체 추출을 실행한다. 다음 코드는 테이블의 전체 내용을 추출하여 파이프로 구분된 CSV 파일에 쓴다. 추출을 수행하기 위해 `pymysql`라이브러리의 `cursor`개체를 사용하여 SELECT 쿼리를 실행한다.

```python
m_query = "SELECT * FROM Orders;"
local_filename = "order_extract.csv"

m_cursor = conn.cursor()
m_cursor.execute(m_query)
results = m_cursor.fetchall()

with open(local_filename, 'w') as fp:
  csv_w = csv.writer(fp, delimiter='|')
  csv_w.writerows(results)

fp.close()
m_cursor.close()
conn.close()
```

이제 CSV 파일이 로컬로 작성되었으므로 나중에 데이터 웨어하우스 또는 다른 대상으로 로드하기 위해 S3 버킷에 업로드해야 한다. S3 버킷에 대한 인증에 사용할 Boto3 라이브러리의 IAM 사용자를 설정한 "클라우드 파일 스토리지 설정"을 상기하시오 [.](https://learning.oreilly.com/library/view/data-pipelines-pocket/9781492087823/ch04.html#setup-cloud-storage)또한 *pipeline.conf* 파일의 `aws_boto_credentials`섹션에 자격 증명을 저장했다. CSV 파일을 S3 버킷에 업로드하는 코드는 다음과 같다.

```python
# load the aws_boto_credentials values
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
access_key = parser.get("aws_boto_credentials", "access_key")
secret_key = parser.get("aws_boto_credentials", "secret_key")
bucket_name = parser.get("aws_boto_credentials", "bucket_name")

s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)

s3_file = local_filename

s3.upload_file(local_filename, bucket_name, s3_file)
```

다음과 같이 스크립트를 실행할 수 있다.

```
(env) $ python extract_mysql_full.py
```

스크립트가 실행되면 `Orders`테이블의 전체 내용이 이제 데이터 웨어하우스 또는 다른 데이터 저장소로 로드되기를 기다리는 S3 버킷에 있는 CSV 파일에 포함된다. 선택한 데이터 저장소에 로드하는 방법에 대한 자세한 내용은 5장을 참조 하시오.

데이터를 점진적으로 추출하려면 스크립트를 약간 변경해야 한다. *Extract_mysql_incremental.py* 라는 이름 의 *extract_mysql_full.py* 복사본을 시작점으로 만드는 것이 좋다 .

먼저 원본 `Orders`테이블에서 추출된 마지막 레코드의 타임스탬프를 찾는다. 그렇게 하려면 데이터 웨어하우스의 `Orders`테이블에서  `MAX(LastUpdated)`값을 쿼리한다. 이 예에서는 Redshift 데이터 웨어하우스를 사용하지만( "Amazon Redshift Warehouse를 대상으로 구성" 참조 ) 선택한 웨어하우스에 동일한 논리를 사용할 수 있다.

Redshift 클러스터와 상호 작용하려면 `psycopg2` 라이브러리를 설치합니다.

```
(env) $ pip install psycopg2
```

다음은 Redshift 클러스터에 연결하고 쿼리하여 `MAX(LastUpdated)`값 을 가져오는 코드이다.
`Orders`테이블 에서:

```python
import psycopg2

# get db Redshift connection info
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
dbname = parser.get("aws_creds", "database")
user = parser.get("aws_creds", "username")
password = parser.get("aws_creds", "password")
host = parser.get("aws_creds", "host")
port = parser.get("aws_creds", "port")

# connect to the redshift cluster
rs_conn = psycopg2.connect(
    "dbname=" + dbname
    + " user=" + user
    + " password=" + password
    + " host=" + host
    + " port=" + port)

rs_sql = """SELECT COALESCE(MAX(LastUpdated),
        '1900-01-01')
        FROM Orders;"""
rs_cursor = rs_conn.cursor()
rs_cursor.execute(rs_sql)
result = rs_cursor.fetchone()

# there's only one row and column returned
last_updated_warehouse = result[0]

rs_cursor.close()
rs_conn.commit()
```

`last_updated_warehouse`에 저장된 값을 사용하여 MySQL 데이터베이스에서 실행된 추출 쿼리를 수정하여 이전에 추출 작업을 실행한 이후 업데이트된 레코드만 `Orders`테이블에서 가져온다. 새 쿼리에는 `last_updated_warehouse`값에 `%s`대해 로 표시되는 자리 표시자가 포함된다. 그런 다음 값은 튜플(데이터 컬렉션을 저장하는 데 사용되는 데이터 유형)으로 커서의 `.execute()` 기능에 전달된다. 이것은 가능한 SQL injection을 피하기 위해 SQL 쿼리에 매개변수를 추가하는 적절하고 안전한 방법이다. 다음은 MySQL 데이터베이스에서 SQL 쿼리를 실행하기 위한 업데이트된 코드 블록이다.

```sql
m_query = """SELECT *
    FROM Orders
    WHERE LastUpdated > %s;"""
local_filename = "order_extract.csv"

m_cursor = conn.cursor()
m_cursor.execute(m_query, (last_updated_warehouse,))
```

증분 추출을 위한 전체 *extract_mysql_incremental.py* 스크립트( `last_updated`값에 Redshift 클러스터 사용)는 다음과 같다.

```python
import pymysql
import csv
import boto3
import configparser
import psycopg2

# get db Redshift connection info
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
dbname = parser.get("aws_creds", "database")
user = parser.get("aws_creds", "username")
password = parser.get("aws_creds", "password")
host = parser.get("aws_creds", "host")
port = parser.get("aws_creds", "port")

# connect to the redshift cluster
rs_conn = psycopg2.connect(
    "dbname=" + dbname
    + " user=" + user
    + " password=" + password
    + " host=" + host
    + " port=" + port)

rs_sql = """SELECT COALESCE(MAX(LastUpdated),
        '1900-01-01')
        FROM Orders;"""
rs_cursor = rs_conn.cursor()
rs_cursor.execute(rs_sql)
result = rs_cursor.fetchone()

# there's only one row and column returned
last_updated_warehouse = result[0]

rs_cursor.close()
rs_conn.commit()

# get the MySQL connection info and connect
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysql_config", "hostname")
port = parser.get("mysql_config", "port")
username = parser.get("mysql_config", "username")
dbname = parser.get("mysql_config", "database")
password = parser.get("mysql_config", "password")

conn = pymysql.connect(host=hostname,
        user=username,
        password=password,
        db=dbname,
        port=int(port))

if conn is None:
  print("Error connecting to the MySQL database")
else:
  print("MySQL connection established!")

m_query = """SELECT *
      FROM Orders
      WHERE LastUpdated > %s;"""
local_filename = "order_extract.csv"

m_cursor = conn.cursor()
m_cursor.execute(m_query, (last_updated_warehouse,))
results = m_cursor.fetchall()

with open(local_filename, 'w') as fp:
  csv_w = csv.writer(fp, delimiter='|')
  csv_w.writerows(results)

fp.close()
m_cursor.close()
conn.close()

# load the aws_boto_credentials values
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
access_key = parser.get(
    "aws_boto_credentials",
    "access_key")
secret_key = parser.get(
    "aws_boto_credentials",
    "secret_key")
bucket_name = parser.get(
    "aws_boto_credentials",
    "bucket_name")

s3 = boto3.client(
    's3',
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key)

s3_file = local_filename

s3.upload_file(
    local_filename,
    bucket_name,
    s3_file)
```

### **MySQL 데이터의 바이너리 로그 복제**

구현하기가 더 복잡하지만 변경 사항을 복제하기 위해 MySQL binlog의 내용을 사용하여 MySQL 데이터베이스에서 데이터를 수집하는 것이 대용량 수집이 필요한 경우 효율적이다.

<aside>
💡 **메모**
Binlog 복제는 CDC(변경 데이터 캡처)의 한 형태이다. 많은 원본 데이터 저장소에는 사용할 수 있는 CDC 형식이 있다.

</aside>

- **MySQL binlog**
: 데이터베이스에서 수행된 모든 작업의 기록을 유지하는 로그
    - 구성 방법에 따라 모든 테이블 생성 또는 수정은 물론 모든 `INSERT`, `UPDATE`및 `DELETE`작업에 대한 세부 정보를 기록한다.
    - 원래 다른 MySQL 인스턴스에 데이터를 복제하기 위한 것이지만 binlog의 내용이 데이터 웨어하우스에 데이터를 수집하려는 데이터 엔지니어에게 유용하기도 하다.

<aside>
💡 ****사전 구축된 프레임워크 사용 고려****
binlog 복제의 복잡성으로 인해 이러한 방식으로 데이터를 수집하려면 오픈 소스 프레임워크 또는 상용 제품을 고려하는 것이 좋다. ["Kafka 및 Debezium 을 사용한 스트리밍 데이터 수집" 에서](https://learning.oreilly.com/library/view/data-pipelines-pocket/9781492087823/ch04.html#stream-w-kafka-debezium) 이러한 옵션 중 하나에 대해 설명 한다. 이 장의 뒷부분에서 언급되는 일부 상용 도구는 binlog 수집도 지원한다.

</aside>

데이터 웨어하우스는 MySQL 데이터베이스가 아닐 가능성이 높기 때문에 내장된 MySQL 복제 기능을 단순히 사용하는 것은 불가능하다. 따라서 비 MySQL 소스에 대한 데이터 수집을 위해 binlog를 사용하려면 여러 단계를 수행해야한다.

1. MySQL 서버에서 binlog를 활성화하고 구성.
2. 초기 전체 테이블 추출 및 로드를 실행.
3. 지속적으로 binlog에서 추출.
4. binlog 추출을 데이터 웨어하우스로 Translate하고 로드.

<aside>
💡 **메모**
3단계는 자세히 설명하지 않지만 수집에 binlog를 사용하려면 먼저 데이터 웨어하우스의 테이블을 MySQL 데이터베이스의 현재 상태로 채운 다음 binlog를 사용하여 후속 변경 사항을 수집해야 한다.

</aside>

바이너리 로깅 활성화 및 구성에 대한 지침은 최신 MySQL binlog 설명서 를 참조하는 것이 가장 좋지만 주요 구성 값을 살펴보자.

**binlog 구성과 관련하여 MySQL 데이터베이스에서 보장해야 하는 두 가지 주요 설정**

1. **바이너리 로깅이 활성화되어 있는지 확인한다.** 
: 일반적으로 기본적으로 활성화되어 있지만 데이터베이스에서 다음 SQL 쿼리를 실행하여 확인할 수 있습니다(정확한 구문은 MySQL 배포판에 따라 다를 수 있음).

```sql
SELECT variable_value as bin_log_status
FROM performance_schema.global_variables
WHERE variable_name='log_bin';
```

바이너리 로깅이 활성화되면 다음이 표시된다. 반환된 상태가 `OFF`이면 해당 버전에 대한 MySQL 설명서를 참조하여 활성화해야 한다.

```
+ — — — — — — — — — — — — — — — — — — -+
| bin_log_status :: |
+ — — — — — — — — — — — — — — — — — — -+
| ON |
+ — — — — — — — — — — — — — — — — — — -+
1 row in set (0.00 sec)
```

1. **바이너리 로깅 형식이 적절하게 설정되었는지 확인한다.** 
: 최신 버전의 MySQL에서는 세 가지 형식이 지원된다.
    - `STATEMENT`
        - binlog에 행을 삽입하거나 수정하는 모든 SQL 문을 기록한다.
        - 한 MySQL 데이터베이스에서 다른 데이터베이스로 데이터를 복제하려는 경우 이 형식이 유용하다.
        데이터를 복제하려면 모든 명령문을 실행하여 데이터베이스 상태를 재현하면 된다. 그러나 추출된 데이터는 다른 플랫폼에서 실행되는 데이터 웨어하우스에 바인딩될 가능성이 높기 때문에 MySQL 데이터베이스에서 생성된 SQL 문은 데이터 웨어하우스와 호환되지 않을 수 있다.
        
    - `ROW`
        - 테이블의 행에 대한 모든 변경 사항이 binlog의 행에 SQL 문이 아니라 행 자체의 데이터로 표시된다.
        - 기본 형식으로 사용된다.
        
    - `MIXED`
        - binlog에 `STATEMENT`-및 `ROW-`formatted 된 레코드를 모두 기록한다.
        - binlog가 다른 용도로 사용되지 않는 한 나중에 `ROW`데이터만 걸러내는 것이 가능하다.
        - binlog가 차지하는 추가 디스크 공간을 감안할 때 , `MIXED`활성화할 필요가 없다.

다음 SQL 쿼리를 실행하여 현재 binlog 형식을 확인할 수 있다.

```sql
SELECT variable_value as bin_log_format
FROM performance_schema.global_variables
WHERE variable_name='binlog_format';
```

명령문은 현재 활성화된 형식을 반환한다.

```
+ — — — — — — — — — — — — — — — — — — — -+
| bin_log_format :: |
+ — — — — — — — — — — — — — — — — — — — -+
| ROW |
+ — — — — — — — — — — — — — — — — — — — -+
1 row in set (0.00 sec)
```

binlog format 및 기타 구성 설정은 일반적으로 MySQL 데이터베이스 인스턴스에 특정한 *my.cnf 파일에 설정된다.* 파일을 열면 다음과 같은 행이 포함된 것을 볼 수 있다.

```
[mysqld]
binlog_format=row
........
```

이제 바이너리 로깅이 `ROW`형식으로 활성화되었으므로 관련 정보를 추출하고 데이터 웨어하우스에 로드할 파일에 저장하는 프로세스를 구축할 수 있다.

binlog에서 가져올 세 가지 `ROW`-형식의 이벤트가 있다. 작업할 이벤트는 다음과 같다.

- `WRITE_ROWS_EVENT`
- `UPDATE_ROWS_EVENT`
- `DELETE_ROWS_EVENT`

다음으로 binlog에서 이벤트를 가져올 차례이다. 시작하는 데 사용할 수 있는 몇 가지 오픈 소스 Python 라이브러리가 있다. 가장 인기 있는 프로젝트 중 하나는 GitHub 에서 찾을 수 있는 `python-mysql-replication`프로젝트 이다.

```
(env) $ pip install mysql-replication
```

binlog의 출력이 어떻게 생겼는지 알아보기 위해 데이터베이스에 연결하고 binlog에서 읽을 수 있다. 이 예에서는 이 섹션 앞부분의 전체 및 증분 수집 예에 대해 *pipeline.conf* 파일에 추가된 MySQL 연결 정보를 사용한다.

<aside>
💡 **메모**
다음 예제는 MySQL 서버의 기본 binlog 파일에서 읽는다. 기본 binlog 파일 이름과 경로는 `log_bin`변수에 설정되며 MySQL 데이터베이스 의 *my.cnf 파일에 저장된다.* 어떤 경우에는 binlog가 시간이 지남에 따라 순환된다(매일 또는 매시간). 
그렇다면 MySQL 관리자가 선택한 로그 회전 방식과 파일 이름 지정 방식에 따라 파일 경로를 결정하고 `BinLogStreamReader`인스턴스를 생성할 때 매개변수 `log_file`에 값으로 전달해야 한다. 자세한 내용 은 해당 `BinLogStreamReader`클래스 의 설명서를 참조.

</aside>

```python
from pymysqlreplication import BinLogStreamReader
from pymysqlreplication import row_event
import configparser
import pymysqlreplication

# get the MySQL connection info
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysql_config", "hostname")
port = parser.get("mysql_config", "port")
username = parser.get("mysql_config", "username")
password = parser.get("mysql_config", "password")

mysql_settings = {
    "host": hostname,
    "port": int(port),
    "user": username,
    "passwd": password
}

b_stream = BinLogStreamReader(
            connection_settings = mysql_settings,
            server_id=100,
            only_events=[row_event.DeleteRowsEvent,
                        row_event.WriteRowsEvent,
                        row_event.UpdateRowsEvent]
            )

for event in b_stream:
    event.dump()

b_stream.close()
```

**코드 샘플에서 인스턴스화된 개체 `BinLogStreamReader`에 대해 몇 가지 주의해야 할 사항**

1. *pipeline.conf* 파일 에 지정된 MySQL 데이터베이스에 연결 하고 특정 binlog 파일에서 읽는다. 
2. `resume_stream=True`설정과 `log_pos`값의 조합은 지정된 지점에서 binlog 읽기를 시작하도록 지시한다. 이 경우 위치는 1400이다. 
3. 마지막으로 `only_events`매개 변수를 사용하여 `DeleteRowsEvent` ,`WriteRowsEvent` , 및 `UpdateRowsEvent`만 읽으라고 한다.

다음으로 스크립트는 모든 이벤트를 반복하고 사람이 읽을 수 있는 형식으로 인쇄한다. `Orders`테이블이 있는 데이터베이스의 경우 다음과 같은 결과가 표시된다.

```
=== WriteRowsEvent ===
Date: 2020-06-01 12:00:00
Log position: 1400
Event size: 30
Read bytes: 20
Table: orders
Affected columns: 3
Changed rows: 1
Values:
--
* OrderId : 1
* OrderStatus : Backordered
* LastUpdated : 2020-06-01 12:00:00

=== UpdateRowsEvent ===
Date: 2020-06-09 12:00:25
Log position: 1401
Event size: 56
Read bytes: 15
Table: orders
Affected columns: 3
Changed rows: 1
Affected columns: 3
Values:
--
* OrderId : 1 => 1
* OrderStatus : Backordered => Shipped
* LastUpdated : 2020-06-01 12:00:00 => 2020-06-09 12:00:25
```

보시다시피 `INSERT`및 `OrderId` 1 `UPDATE`을 나타내는 두 개의 이벤트가 있으며 이는 *표 4-3* 에 나와 있다. 
이 가상의 예에서 두 개의 순차적 binlog 이벤트는 며칠 떨어져 있지만 실제로는 데이터베이스의 모든 변경 사항을 나타내는 수많은 이벤트가 둘 사이에 있을 것이다.

<aside>
💡 **메모**
시작 위치를 알려주는 `BinLogStreamReader`의 값 `log_pos`은 다음 추출이 실행될 때 선택할 위치를 추적하기 위해 고유한 테이블의 어딘가에 저장해야 하는 값이다. 추출이 시작될 때 읽을 수 있고 쓸 수 있는 데이터 웨어하우스의 로그 테이블에 값을 저장하는 것이 가장 좋다. 이 값은 완료될 때 최종 이벤트의 위치 값과 함께 저장된다.

</aside>

코드 샘플은 이벤트가 사람이 읽을 수 있는 형식으로 어떻게 표시되는지 보여주지만 출력을 데이터 웨어하우스에 쉽게 로드할 수 있도록 하려면 몇 가지 작업을 더 수행해야 한다.

- **데이터를 구문 분석하고 다른 형식으로 쓴다.** 
: 로드를 단순화하기 위해 다음 코드 샘플에서는 CSV 파일의 행에 각 이벤트를 쓴다.
    - 첫 번째 변경 사항을 해결하기 위해 `.dump()`함수를 사용하는 대신 이벤트 속성을 구문 분석하고 CSV 파일에 쓴다.
    
- **추출 및 로드하려는 테이블당 하나의 파일을 작성한다.** 
: 예제 binlog에는  `Orders`테이블과 관련된 이벤트만 포함되지만 실제 binlog에는 다른 테이블과 관련된 이벤트도 포함될 가능성이 높다.
    - 각 테이블에 대한 파일을 작성하는 대신 단순성을 위해 `Orders`테이블과 관련된 이벤트만 *orders_extract.csv* 라는 파일에 작성 한다.

최종 코드 샘플의 마지막 단계는 CSV 파일을 S3 버킷에 업로드하여 5장 에 자세히 설명된 대로 데이터 웨어하우스에 로드 할 수 있다.

```python
from pymysqlreplication import BinLogStreamReader
from pymysqlreplication import row_event
import configparser
import pymysqlreplication
import csv
import boto3

# get the MySQL connection info
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
hostname = parser.get("mysql_config", "hostname")
port = parser.get("mysql_config", "port")
username = parser.get("mysql_config", "username")
password = parser.get("mysql_config", "password")

mysql_settings = {
    "host": hostname,
    "port": int(port),
    "user": username,
    "passwd": password
}

b_stream = BinLogStreamReader(
            connection_settings = mysql_settings,
            server_id=100,
            only_events=[row_event.DeleteRowsEvent,
                        row_event.WriteRowsEvent,
                        row_event.UpdateRowsEvent]
            )

order_events = []

for binlogevent in b_stream:
  for row in binlogevent.rows:
    if binlogevent.table == 'orders':
      event = {}
      if isinstance(
            binlogevent,row_event.DeleteRowsEvent
        ):
        event["action"] = "delete"
        event.update(row["values"].items())
      elif isinstance(
            binlogevent,row_event.UpdateRowsEvent
        ):
        event["action"] = "update"
        event.update(row["after_values"].items())
      elif isinstance(
            binlogevent,row_event.WriteRowsEvent
        ):
        event["action"] = "insert"
        event.update(row["values"].items())

      order_events.append(event)

b_stream.close()

keys = order_events[0].keys()
local_filename = 'orders_extract.csv'
with open(
        local_filename,
        'w',
        newline='') as output_file:
    dict_writer = csv.DictWriter(
                output_file, keys,delimiter='|')
    dict_writer.writerows(order_events)

# load the aws_boto_credentials values
parser = configparser.ConfigParser()
parser.read("pipeline.conf")
access_key = parser.get(
                "aws_boto_credentials",
                "access_key")
secret_key = parser.get(
                "aws_boto_credentials",
                "secret_key")
bucket_name = parser.get(
                "aws_boto_credentials",
                "bucket_name")

s3 = boto3.client(
    's3',
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key)

s3_file = local_filename

s3.upload_file(
    local_filename,
    bucket_name,
    s3_file)
```

실행 후 *orders_extract.csv* 는 다음과 같다.

```
insert|1|Backordered|2020-06-01 12:00:00
update|1|Shipped|2020-06-09 12:00:25
```