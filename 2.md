# 2장. 최신 데이터 인프라

생성일: 2022년 3월 9일 오후 12:05

### **데이터 소스의 다양성**

: 대부분의 조직에는 데이터 소스가 수백 개는 아니더라도 수십 개 있다. 데이터 소스는 여러 차원에 따라 다르다.
****

![https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492087823/files/assets/dppr_0201.png](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492087823/files/assets/dppr_0201.png)

**그림 2-1. 최신 데이터 인프라의 핵심 구성 요소**

- **소스 시스템 소유권**
: 분석 팀은 조직에서 구축하고 소유한 소스 시스템과 타사 도구 및 공급업체에서 데이터를 수집하는 것이 일반적이다.
    - ***데이터 수집*** 
    : 한 소스에서 데이터를 추출하여 다른 소스로 로드하는 것을 의미한다.
    ex) PostgreSQL(= Postgres), Google Analytics
    - **소스 시스템의 소유권을 이해하는 것이 중요한 이유**
        - 내부적으로 구축된 시스템은 분석 팀에게 사용 가능한 데이터와 액세스 방법을 사용자 지정할 수 있는 더 많은 기회를 제공한다.
        - 타사 데이터 소스의 경우 액세스할 수 있는 데이터와 액세스 방법이 제한될 수 있다. 대부분의 공급업체에서 REST API를 사용할 수 있지만 SQL 데이터베이스 형식으로 데이터에 직접 액세스할 수 있는 공급업체는 거의 없다.
        

![https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492087823/files/assets/dppr_0202.png](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492087823/files/assets/dppr_0202.png)

**그림 2-2. 여러 소스의 데이터가 S3 버킷에 로드된 다음 Redshift 데이터베이스에 로드되는 간단한 파이프라인**

- **수집 인터페이스 및 데이터 구조**
: 각 인터페이스와 데이터 구조는 고유한 과제와 기회를 제공한다. 잘 구조화된 데이터는 작업하기 쉬운 경우가 많지만 일반적으로 애플리케이션이나 웹사이트를 위해 구조화된다. 데이터 수집 외에도 분석 프로젝트에 더 적합한 구조로 정리하고 변환하려면 파이프라인의 추가 단계가 필요할 수 있다.
    
    
    - **데이터에 대한 인터페이스**
    • Postgres 또는 MySQL 데이터베이스와 같은 애플리케이션 뒤에 있는 데이터베이스
    • REST API와 같은 시스템 상단의 추상화 계층
    • Apache Kafka와 같은 스트림 처리 플랫폼
    • 로그, 쉼표로 구분된 값(CSV) 파일 및 기타 플랫 파일이 포함된 공유 네트워크 파일 시스템 또는 클라우드 스토리지 버킷
    • 데이터 웨어하우스 또는 데이터 레이크
    • HDFS 또는 HBase 데이터베이스의 데이터
    
    - **데이터 구조**
    • REST API의 JSON
    • MySQL 데이터베이스의 잘 구성된 데이터
    • MySQL 데이터베이스 테이블의 열 내의 JSON
    • 반구조화된 로그 데이터
    • CSV, 고정 너비 형식(FWF) 및 기타 플랫 파일 형식
    • 플랫 파일의 JSON
    • Kafka의 스트림 출력
    
    - JSON과 같은 반정형 데이터
    : 점점 보편화되고 있으며 속성-값 쌍의 구조와 객체의 중첩이라는 장점 있다. 그러나 관계형 데이터베이스와 달리 동일한 데이터 세트의 각 개체가 동일한 구조를 가질 것이라는 보장은 없다. 이 책의 뒷부분에서 볼 수 있듯이 파이프라인에서 누락되거나 불완전한 데이터를 처리하는 방법은 상황에 따라 달라지며 데이터 구조의 경직성이 감소함에 따라 점점 더 필요하다.
    
    - 비정형 데이터
    : 일부 분석 작업에 일반적이다. 예를 들어, 자연어 처리(NLP) 모델은 훈련 및 검증을 위해 방대한 양의 자유 텍스트 데이터가 필요하며, 컴퓨터 비전(CV) 프로젝트에는 이미지와 비디오 콘텐츠가 필요하다. 웹 페이지에서 데이터를 스크랩하는 것에는 웹 페이지의 반구조화된 HTML 마크업 외에도 웹에서 무료 텍스트 데이터가 필요하다.

- **데이터 볼륨**
: 크고 작은 데이터 세트를 함께 수집하고 모델링하는 것이 일반적 이다. 파이프라인의 각 단계에서 설계 결정은 데이터 볼륨을 고려해야 하지만 높은 볼륨이 높은 가치를 의미하지는 않는다.
    - *대용량(**high volume)* 이란 ? 
    : 파이프라인과 관련하여 고용량 및 저용량 데이터 세트의 이진적인 정의보다 스펙트럼 측면에서 생각하는 것이 좋다.
    
- **데이터 정리 및 유효성**
: 데이터 소스에 큰 다양성이 있는 것처럼 소스 데이터의 품질도 크게 다르다. 따라서 소스 데이터의 한계와 결함을 이해하고 파이프라인의 적절한 섹션에서 해결하는 것이 중요하다.
    - **"messy data"의 일반적인 특성**
    • 중복되거나 모호한 기록
    • 고아 레코드
    • 불완전하거나 누락된 기록
    • 텍스트 인코딩 오류
    • 일치하지 않는 형식(예: 대시가 있거나 없는 전화번호)
    • 레이블이 잘못 지정되거나 레이블이 지정되지 않은 데이터
    
    - **데이터 청정도와 유효성을 보장하는 주요 특성과 접근 방식**
        - **최악을 가정하고 최고를 기대하라**
        : 입력 데이터 세트에 수많은 유효성 및 일관성 문제가 포함되어 있지만 깨끗한 출력을 위해 데이터를 식별하고 정리하는 파이프라인을 구축한다고 가정한다.
        
        - **그렇게 하는 데 가장 적합한 시스템에서 데이터를 정리하고 검증한다.**
        : 파이프라인의 나중까지 데이터 정리를 기다리는 것이 더 나은 경우도 있다. 따라서 청소 및 검증 프로세스를 서두르지 말고 올바른 작업에 올바른 도구를 사용하는 것이 중요하다.
            
            최신 파이프라인은 데이터 웨어하우징에 대해
            
            - ETL(추출-변환-로드) 접근 방식보다
            - ELT(추출-로드-변환)를 따르는 경향이 있음.
            : 데이터를 원시 형태로 데이터 레이크에 로드하고 나중에 파이프라인에서 구조화 및 정리에 대해 걱정하는 것이 때때로 최적일 수 있다.
            
        - **자주 확인**
        : 파이프라인 초기에 데이터를 정리하지 않더라도 유효성을 검사하기 위해 파이프라인이 끝날 때까지 기다리지 말아야 한다. 끝날 때 까지 기다린다면, 문제가 어디에서 잘못되었는지 결정하기가 훨씬 더 어려울 수 있기 때문이다. 또한 반대로, 파이프라인 초기에 한 번만 검증하는 것은 지양 해야 한다.
    
- **소스 시스템의 대기 시간 및 대역폭**
    - 최신 데이터 스택의 일반적인 사용 사례
    : 소스 시스템에서 대량의 데이터를 자주 추출해야 하는 필요성이 존재 한다. 그러나 이것은 API 속도 제한, 연결 시간 초과, 느린 다운로드, 시스템에 가해지는 부담으로 인해 불만을 품은 소스 시스템 소유자 등의 문제를 야기할 수 있다.

---

### **클라우드 데이터 웨어하우스 및 데이터 레이크**

: 지난 3년 동안 분석 및 데이터 웨어하우징 환경을 변화시킨 세 가지10년, 그리고 모두 주요 퍼블릭 클라우드 제공업체(Amazon, Google 및 Microsoft)의 등장과 관련이 있다. 이러한 변화는 데이터 웨어하우스에 새로운 생명을 불어넣고 데이터 레이크의 개념을 도입했다. 동일한 데이터 생태계에 데이터 웨어하우스와 데이터 레이크 모두를 위한 장소가 있으며 데이터 파이프라인은 종종 둘 사이에서 데이터를 이동한다.

• 클라우드에서 데이터 파이프라인, 데이터 레이크, 웨어하우스 및 분석 처리를 쉽게 구축하고 배포할 수 있다. 
• 클라우드의 지속적인 드롭인 스토리지 비용.
• Amazon Redshift, Snowflake 및 Google Big Query와 같은 확장성이 뛰어난 열 기반 데이터베이스의 등장.

- ***데이터 웨어하우스**
:* 쿼리에 답하는 것과 관련된 분석 및 기타 활동을 지원하기 위해 서로 다른 시스템 의 데이터가 저장되고 모델링되는 데이터베이스
    - 데이터 웨어하우스의 데이터는 보고 및 분석 쿼리를 위해 구조화되고 최적화된다.
    
- ***데이터 레이크*** 
: 데이터가 저장되지만 데이터 웨어하우스의 구조나 쿼리 최적화가 없는 곳
    - 여기에는 많은 양의 데이터와 다양한 데이터 유형이 포함될 수 있다. 
    : 예를 들어, 단일 데이터 레이크에는 텍스트 파일로 저장된 블로그 게시물 모음, 관계형 데이터베이스에서 추출한 플랫 파일, 산업 시스템의 센서에 의해 생성된 이벤트가 포함된 JSON 개체가 포함될 수 있다.
    - 보고 및 분석을 위해 이러한 데이터를 쿼리하는 데 최적화되어 있지는 않지만 표준 데이터베이스처럼 구조화된 데이터를 저장할 수도 있다.
    

---

### **데이터 수집 도구**

: 한 시스템에서 다른 시스템으로 데이터를 수집해야 하는 필요성은 거의 모든 데이터 파이프라인에 공통적이다. 최신 데이터 인프라에서 다양한 상용 및 오픈 소스 도구를 사용할 수 있습니다. 다음을 포함하여 이러한 도구 및 프레임워크 중 가장 일반적인 몇 가지에 대해 설명한다.

• Singer
• Stitch
• Fivetran

---

### **데이터 변환 및 모델링 도구**

: 이 장의 대부분은 소스와 대상 간의 데이터 이동(데이터 수집)에 중점을 두었지만 데이터 파이프라인과 데이터 이동에는 훨씬 더 많은 내용이 있다. 파이프라인은 또한 기계 학습, 분석 및 보고와 같은 새로운 목적을 위해 데이터를 변환하고 모델링하는 작업으로 구성된다.

*데이터 모델링* 과 *데이터 변환* 이라는 용어 는 종종 같은 의미로 사용된다. 하지만 구분 가능 하다.

- **데이터 변환**
: ETL 또는 ELT 프로세스에서 *T* 로 표시되는 광범위한 용어
    - ex) 변환은 테이블에 저장된 타임스탬프를 한 시간대에서 다른 시간대로 변환하는 것, 일부 비즈니스 논리를 통해 집계 및 필터링되는 여러 소스 열에서 새 메트릭을 만드는 것 등
    - 데이터 변환은 종종 상황에 따라 다르며 SQL 또는 Python과 같은 데이터 엔지니어 및 데이터 분석가에게 친숙한 언어로 작성할 수 있다.
    
- **데이터 모델링**
: 보다 구체적인 데이터 변환 유형
    - 데이터 모델은 데이터 분석을 위해 이해되고 최적화된 형식으로 데이터를 구조화하고 정의한다. 또 일반적으로 데이터 웨어하우스에서 하나 이상의 테이블로 표시된다.
    - 분석 및 보고에 사용되는 데이터 모델은 일반적으로 SQL 또는 point-and-click user interfaces를 통해 정의 및 작성된다.
    

---

### **워크플로 오케스트레이션 플랫폼(workflow Orchestration Platforms)**

: 조직에서 데이터 파이프라인의 복잡성과 수가 증가함에 따라 데이터 인프라 에 *워크플로 오케스트레이션 플랫폼 을 도입하는 것* 이 중요하다.이러한 플랫폼은 파이프라인에서 작업의 일정 및 흐름을 관리하기 때문이다. 

Python으로 작성된 데이터 수집에서 하루 종일 특정 순서로 실행되어야 하는 SQL로 작성된 데이터 변환에 이르는 12가지 작업이 있는 파이프라인를 예로 생각해보면, 각 작업 간의 종속성을 예약하고 관리하는 것은 간단한 문제가 아니다. 고맙게도 고통을 완화할 수 있는 워크플로 오케스트레이션 플랫폼이 많이 있다.

*워크플로 오케스트레이션 플랫폼(workflow Orchestration Platforms)은 
워크플로 관리 시스템* (WMS), *오케스트레이션 플랫폼* 또는 *오케스트레이션 프레임워크* 라고도 한다. 

- **Apache Airflow, Luigi 및 AWS Glue와 같은 일부 플랫폼**
: 보다 일반적인 사용 사례를 위해 설계되었으므로 다양한 데이터 파이프라인에 사용된다.
- **Kubeflow Pipelines와 같은 다른 것들**
: 보다 구체적인 사용 사례 및 플랫폼을 위해 설계되었다.
 (Kubeflow Pipelines의 경우 Docker 컨테이너에 구축된 기계 학습 워크플로)

- **방향성 비순환 그래프**
거의 모든 최신 오케스트레이션 프레임워크는 파이프라인에서 작업의 흐름과 종속성을 그래프로 나타낸다. 그러나 파이프라인 그래프에는 **몇 가지 특정 제약 조건**이 있다.
    1. **파이프라인 단계는 항상 *지시* (always *directed)* 된다.** 
    = 일반 작업 또는 여러 작업으로 시작하여 특정 작업으로 끝난다. 
    : 모든 종속 작업이 성공적으로 완료되기 전에 작업이 실행되지 않도록 한다.
    2. **파이프라인 그래프는 또한 *acyclic* 이어야 한다.** 
    = 작업은 이전에 완료된 작업을 다시 가리킬 수 없다. 즉, 다시 순환할 수 없다. 
    : 파이프라인이 끝없이 실행될 수 있기 때문

이러한 두 가지 제약 조건을 염두에 두고
 오케스트레이션 파이프라인은 DaG(방향성 비순환 그래프)라는 그래프를 생성한다. 

![https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492087823/files/assets/dppr_0203.png](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492087823/files/assets/dppr_0203.png)

**그림 2-3. 네 가지 작업이 있는 DAG. 
작업 A가 완료되면 작업 B와 작업 C가 실행된다. 둘 다 완료되면 작업 D가 실행된다.**

DAG는 작업의 논리가 정의된 곳이 아니라 작업 집합을 나타낸다. 오케스트레이션 플랫폼은 모든 종류의 작업을 실행할 수 있다.

예를 들어, 세 가지 작업이 있는 데이터 파이프라인을 생각해 보자.

오케스트레이션 플랫폼은 각 작업을 실행하지만, 작업의 논리는 데이터 인프라 전반에 걸쳐 서로 다른 시스템에서 실행되는 SQL 및 Python 코드로 존재한다.

![https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492087823/files/assets/dppr_0204.png](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492087823/files/assets/dppr_0204.png)

**그림 2-4. SQL 데이터베이스에서 데이터를 추출하고 Python 스크립트를 사용하여 데이터를 정리 및 재구성한 다음 결과 데이터를 데이터 웨어하우스에 로드하기 위해 순차적으로 실행되는 세 가지 작업이 포함된 DAG.**

• **Query_and_Exract**
: 첫 번째는 관계형 데이터베이스에서 데이터를 쿼리하고 결과를 CSV 파일에 저장하는 SQL 스크립트를 실행

• **Clean_and_Reshape**
: 두 번째는 CSV 파일을 로드하고 정리한 다음 파일의 새 버전을 저장하기 전에 데이터를 재구성하는 Python 스크립트를 실행

• **Load_Clean_Data**
: 마지막으로 SQL에서 COPY 명령을 실행하는 세 번째 작업은 두 번째 작업에서 생성된 CSV를 Snowflake 데이터 웨어하우스로 로드

---

### **데이터 인프라 사용자 정의**

: 앞서 언급했듯이 조직의 문화와 리소스에 따라 대부분의 데이터 인프라를 자체적으로 구축하거나 대신 SaaS 공급업체에 의존하는 것이 좋다. 중요한 것은 제약 조건(달러, 엔지니어링 리소스, 보안 및 법적 위험 허용 범위)과 그에 따른 절충점을 이해하는 것이다.